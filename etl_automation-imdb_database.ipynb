{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Automation - IMDb Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caio\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "import logging\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging Configuration\n",
    "log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.DEBUG, format=log_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition for Data Scraping at the website\n",
    "DATABASE = \"imdb_data.db\"\n",
    "DIRECTORY = \"data\"\n",
    "URL = \"https://datasets.imdbws.com/\"\n",
    "FILES = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Data Extraction\n",
    "def extract(\n",
    "    data_url: str = URL,\n",
    "    files: list = FILES,\n",
    "    data_dir: str = DIRECTORY,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        url = data_url + file\n",
    "        dir_path = os.path.join(data_dir, file)\n",
    "\n",
    "        if not os.path.exists(dir_path):\n",
    "            logging.info(f\"Downloading {file}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                with open(dir_path, \"wb\") as f:\n",
    "                    shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "                logging.info(f\"{file} successfully downloaded!\")\n",
    "\n",
    "            else:\n",
    "                logging.error(f\"Fail downloading {file}! Status Code: {response.status_code}\")\n",
    "\n",
    "            del response\n",
    "        else:\n",
    "            logging.info(f\"{file} already exists! Skipping download...\")\n",
    "\n",
    "    logging.info(\"Download Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Data Transform\n",
    "def transform(\n",
    "    files: list = FILES,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = \"data\"\n",
    "    df_dir = os.path.join(data_dir, \"processed\")\n",
    "\n",
    "    os.makedirs(df_dir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        dir_path = os.path.join(data_dir, file)\n",
    "\n",
    "        if os.path.isfile(dir_path) and file.endswith(\".gz\"):\n",
    "            logging.debug(f\"Reading and processing data {file}...\")\n",
    "\n",
    "            df = pd.read_csv(\n",
    "                dir_path,\n",
    "                sep=\"\\t\",\n",
    "                compression=\"gzip\",\n",
    "                low_memory=False,\n",
    "                nrows=1_000\n",
    "                #chunksize=1_000_000\n",
    "            )\n",
    "\n",
    "            df.replace({\"\\\\N\": None}, inplace=True)\n",
    "\n",
    "            dest_dir = os.path.join(df_dir, file[:-3])\n",
    "            df.to_csv(dest_dir, sep=\"\\t\", index=False)\n",
    "\n",
    "            logging.debug(f\"Processing done for {file}. Processed file saved in {dest_dir}!\")\n",
    "\n",
    "            #Remove file after processing\n",
    "            #os.remove(dir_path)\n",
    "\n",
    "    logging.info(\"All files processed and saved on folder 'processed'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Data Load\n",
    "def load(\n",
    "    conn\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    df_dir = os.path.join(\"data\", \"processed\")\n",
    "\n",
    "    files = os.listdir(df_dir)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(df_dir, file)\n",
    "\n",
    "        if os.path.isfile(file_path) and file.endswith(\".tsv\"):\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "            table_name = os.path.splitext(file)[0]\n",
    "            table_name = table_name.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "            df.to_sql(table_name, conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "            logging.info(f\"File {file} saved as table {table_name} to the database.\")\n",
    "\n",
    "            #Remove file path after load\n",
    "            #os.remove(file_path)\n",
    "\n",
    "    logging.info(\"All files have been saved to the database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Data Testing and Set Analytic Views\n",
    "def analytic_views(\n",
    "    conn\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docstring.\n",
    "    \"\"\"\n",
    "\n",
    "    title_analytics = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS title_analytics AS\n",
    "\n",
    "    WITH \n",
    "    participants AS (\n",
    "        SELECT\n",
    "            tconst,\n",
    "            COUNT(DISTINCT nconst) as qtParticipants\n",
    "        \n",
    "        FROM title_principals\n",
    "        \n",
    "        GROUP BY 1\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        tb.tconst,\n",
    "        tb.titleType,\n",
    "        tb.originalTitle,\n",
    "        tb.startYear,\n",
    "        tb.endYear,\n",
    "        tb.genres,\n",
    "        tr.averageRating,\n",
    "        tr.numVotes,\n",
    "        tp.qtParticipants\n",
    "\n",
    "    FROM title_basics tb \n",
    "\n",
    "    LEFT JOIN title_ratings tr\n",
    "        ON tr.tconst = tb.tconst\n",
    "\n",
    "    LEFT JOIN participants tp\n",
    "        ON tp.tconst = tb.tconst\n",
    "    \"\"\"\n",
    "\n",
    "    participants_analytics = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS participants_analytics AS\n",
    "\n",
    "    SELECT\n",
    "        tp.nconst,\n",
    "        tp.tconst,\n",
    "        tp.ordering,\n",
    "        tp.category,\n",
    "        tb.genres\n",
    "\n",
    "    FROM title_principals tp\n",
    "\n",
    "    LEFT JOIN title_basics tb\n",
    "        ON tb.tconst = tp.tconst\n",
    "    \"\"\"\n",
    "\n",
    "    queries = [\n",
    "        \"DROP TABLE IF EXISTS title_analytics\",\n",
    "        \"DROP TABLE IF EXISTS participants_analytics\",\n",
    "        title_analytics,\n",
    "        participants_analytics\n",
    "    ]\n",
    "\n",
    "    logging.info(\"Saving analytical tables to the database...\")\n",
    "\n",
    "    for query in queries:\n",
    "        conn.execute(query)\n",
    "        \n",
    "\n",
    "    logging.info(\"Analytical tables successfully created!\")\n",
    "\n",
    "    print(\"End of ETL Process!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0. ETL Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 21:44:26,870 - INFO - Downloading name.basics.tsv.gz...\n",
      "2025-06-09 21:44:26,874 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:44:27,405 - DEBUG - https://datasets.imdbws.com:443 \"GET /name.basics.tsv.gz HTTP/1.1\" 200 285721152\n",
      "2025-06-09 21:44:46,086 - INFO - name.basics.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:44:46,087 - INFO - Downloading title.akas.tsv.gz...\n",
      "2025-06-09 21:44:46,089 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:44:46,146 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.akas.tsv.gz HTTP/1.1\" 200 454634118\n",
      "2025-06-09 21:45:15,896 - INFO - title.akas.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:45:15,896 - INFO - Downloading title.basics.tsv.gz...\n",
      "2025-06-09 21:45:15,898 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:45:15,946 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.basics.tsv.gz HTTP/1.1\" 200 206922592\n",
      "2025-06-09 21:45:29,500 - INFO - title.basics.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:45:29,501 - INFO - Downloading title.crew.tsv.gz...\n",
      "2025-06-09 21:45:29,503 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:45:29,550 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.crew.tsv.gz HTTP/1.1\" 200 76513876\n",
      "2025-06-09 21:45:34,269 - INFO - title.crew.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:45:34,269 - INFO - Downloading title.episode.tsv.gz...\n",
      "2025-06-09 21:45:34,271 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:45:34,326 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.episode.tsv.gz HTTP/1.1\" 200 49713394\n",
      "2025-06-09 21:45:37,227 - INFO - title.episode.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:45:37,228 - INFO - Downloading title.principals.tsv.gz...\n",
      "2025-06-09 21:45:37,230 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:45:37,279 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.principals.tsv.gz HTTP/1.1\" 200 719177134\n",
      "2025-06-09 21:46:25,823 - INFO - title.principals.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:46:25,824 - INFO - Downloading title.ratings.tsv.gz...\n",
      "2025-06-09 21:46:25,826 - DEBUG - Starting new HTTPS connection (1): datasets.imdbws.com:443\n",
      "2025-06-09 21:46:25,883 - DEBUG - https://datasets.imdbws.com:443 \"GET /title.ratings.tsv.gz HTTP/1.1\" 200 7933622\n",
      "2025-06-09 21:46:26,558 - INFO - title.ratings.tsv.gz successfully downloaded!\n",
      "2025-06-09 21:46:26,558 - INFO - Download Finished!\n",
      "2025-06-09 21:46:26,559 - DEBUG - Reading and processing data name.basics.tsv.gz...\n",
      "2025-06-09 21:46:26,582 - DEBUG - Processing done for name.basics.tsv.gz. Processed file saved in data\\processed\\name.basics.tsv!\n",
      "2025-06-09 21:46:26,583 - DEBUG - Reading and processing data title.akas.tsv.gz...\n",
      "2025-06-09 21:46:26,600 - DEBUG - Processing done for title.akas.tsv.gz. Processed file saved in data\\processed\\title.akas.tsv!\n",
      "2025-06-09 21:46:26,601 - DEBUG - Reading and processing data title.basics.tsv.gz...\n",
      "2025-06-09 21:46:26,620 - DEBUG - Processing done for title.basics.tsv.gz. Processed file saved in data\\processed\\title.basics.tsv!\n",
      "2025-06-09 21:46:26,621 - DEBUG - Reading and processing data title.crew.tsv.gz...\n",
      "2025-06-09 21:46:26,638 - DEBUG - Processing done for title.crew.tsv.gz. Processed file saved in data\\processed\\title.crew.tsv!\n",
      "2025-06-09 21:46:26,638 - DEBUG - Reading and processing data title.episode.tsv.gz...\n",
      "2025-06-09 21:46:26,656 - DEBUG - Processing done for title.episode.tsv.gz. Processed file saved in data\\processed\\title.episode.tsv!\n",
      "2025-06-09 21:46:26,656 - DEBUG - Reading and processing data title.principals.tsv.gz...\n",
      "2025-06-09 21:46:26,677 - DEBUG - Processing done for title.principals.tsv.gz. Processed file saved in data\\processed\\title.principals.tsv!\n",
      "2025-06-09 21:46:26,678 - DEBUG - Reading and processing data title.ratings.tsv.gz...\n",
      "2025-06-09 21:46:26,696 - DEBUG - Processing done for title.ratings.tsv.gz. Processed file saved in data\\processed\\title.ratings.tsv!\n",
      "2025-06-09 21:46:26,697 - INFO - All files processed and saved on folder 'processed'!\n",
      "2025-06-09 21:46:26,733 - INFO - File name.basics.tsv saved as table name_basics to the database.\n",
      "2025-06-09 21:46:26,758 - INFO - File title.akas.tsv saved as table title_akas to the database.\n",
      "2025-06-09 21:46:26,781 - INFO - File title.basics.tsv saved as table title_basics to the database.\n",
      "2025-06-09 21:46:26,801 - INFO - File title.crew.tsv saved as table title_crew to the database.\n",
      "2025-06-09 21:46:26,820 - INFO - File title.episode.tsv saved as table title_episode to the database.\n",
      "2025-06-09 21:46:26,841 - INFO - File title.principals.tsv saved as table title_principals to the database.\n",
      "2025-06-09 21:46:26,860 - INFO - File title.ratings.tsv saved as table title_ratings to the database.\n",
      "2025-06-09 21:46:26,861 - INFO - All files have been saved to the database!\n",
      "2025-06-09 21:46:26,861 - INFO - Saving analytical tables to the database...\n",
      "2025-06-09 21:46:26,873 - INFO - Analytical tables successfully created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ETL Process!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract()\n",
    "    transform()\n",
    "\n",
    "    conn = sqlite3.connect(DATABASE)\n",
    "\n",
    "    load(conn=conn)\n",
    "    analytic_views(conn=conn)\n",
    "\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
